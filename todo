- make query better
  - one thing I notice is even we use AAFI to get more factors for "nasa shuttle"
  the facotrs I get are like prefixes ("nasa shuttle", "nasa sh") etc 
  - what i want is to also get 
  "a shuttle", "shuttle" etc
  - I don't want to rely on "split the query at whitespace (tho that could be a useful hack)
  - But I want to get More data out of the query
- name, doc name is currently a number, which becomes a text key in the JSON
let's convert the number to its base36 rep as its a string anyway. saves a few bytes
- Investigate around our best prec (param space) to see if we can increase rec and keep cost low.

- make a tool, a separate file, program, to investigate nature of dictionary (frequency, etc)
  - investigate difference in dict between a high prec (best 83/ etc), and a high recall (high score) one. What's the diff, maybe we can find ways to increase rec and keep prec that are inexpensive.
  - find useless entries that we can prune
  - limit number of entries in dict?
  - limit entries added per indexed document?
  - what if we trim dictionary entries of surrounding whitespace, and also trim the word when we check for it. It might save some space as well as some time?
- add option to use on disk dictionary (sirdb)
- add .save (to save dictionary to disk) to Q&A mode
- evaluate out of memory (range error) condition using bigmap (which shouldn't happen)
  - try to double add (via some random "code id" now that we have removed code id)
  - limit max old space size and try to recreate the out of memory condition
  - then increase max old space size and see if we can remove the out of memory condition
  - also add debug line to bigmap code (done!) that shows the condition of the child dictionary when the set fails
- remove all old code
- purge and prune dictionary to get it as small as possible while maintaining results
- make each count/score object a typed array?

// mid
  - make chunks dependent on byte size rather than value length 
  (and stay under 2gb per file limit)
  - then go forward and using what we have try to prune more aggressively and on a metric like score, or something, to create even more improvements
  - snippets are important to evaluate the document
  - multicore indexing (if available), which would be split across a list of documents and merge, complexity from having a shared dictionary, IPC comms?

// old
  - don't pre-emptively optimise, keep the algorithm loose and open before it's thoroughly tested
  - add spanish
  - things we can try to improve performance:
    - the usual parameters, cover, stopping condition, score calculation,1
    - add new things like tf-idf for factors, cosine similarity (on what I don't yet know), boosting doc in (doc, query) based on how many times that doc has been the first clicked result for that query
    - joining documents across factors rather than just summing scores.
    - or summing scores, but multiplying across those factors that share a common document
  - create a way to do snippets (contextual snippets to put the search result in context of the document)
  - create a way to do instant (basically just indexing the queries, and querying those against the current query as it is typed)



