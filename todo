- remove ==== and other punctuation not covered by unicode regex categories
  - the unicode property for this is 'S' so /\p{S}/u will match symbols LET'S ROCK IT! :p ;P ;) xx
  - i think i should remove all sequences of 2 or more symbols, any any "other symbols" but
  leave single instances to preserve stuff like "$1.50" and "1/2"
  - and check effect on dictionary size
  - also ensure 140 is preserved
  - 140 is NOT preserved. It is a little bit less. That's not good. 
  - Investigate further see if I can find a better "simplify" function to improve rate, but still remove symbols. 
- investigate nature of dictionary (frequency, etc)
  - limit number of entries in dict?
  - limit entries added per indexed document?
  - what if we trim dictionary entries of surrounding whitespace, and also trim the word when we check for it. It might save some space as well as some time?
- evaluate out of memory (range error) condition using bigmap (which shouldn't happen)
  - try to double add (via some random "code id" now that we have removed code id)
  - limit max old space size and try to recreate the out of memory condition
  - then increase max old space size and see if we can remove the out of memory condition
  - also add debug line to bigmap code (done!) that shows the condition of the child dictionary when the set fails
- remove all old code
- purge and prune dictionary to get it as small as possible while maintaining results
- make each count/score object a typed array?

// mid
  - make chunks dependent on byte size rather than value length 
  (and stay under 2gb per file limit)
  - then go forward and using what we have try to prune more aggressively and on a metric like score, or something, to create even more improvements
  - snippets are important to evaluate the document
  - multicore indexing (if available), which would be split across a list of documents and merge, complexity from having a shared dictionary, IPC comms?

// old
  - don't pre-emptively optimise, keep the algorithm loose and open before it's thoroughly tested
  - add spanish
  - things we can try to improve performance:
    - the usual parameters, cover, stopping condition, score calculation,1
    - add new things like tf-idf for factors, cosine similarity (on what I don't yet know), boosting doc in (doc, query) based on how many times that doc has been the first clicked result for that query
    - joining documents across factors rather than just summing scores.
    - or summing scores, but multiplying across those factors that share a common document
  - create a way to do snippets (contextual snippets to put the search result in context of the document)
  - create a way to do instant (basically just indexing the queries, and querying those against the current query as it is typed)



